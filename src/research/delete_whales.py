'''

This script deletes the top 100 largest files from our sampled dataset in s3. 
The sec filings sizes are highly skewed, with the biggest files being 10-100x larger than the average file size.. 
Uses the top100largest.txt file generated by the following command:

aws s3api list-objects-v2 \
  --bucket edgar-edge-raw \
  --prefix sample/raw/ \
  --query "reverse(sort_by(Contents, &Size))[:100].Key" \
  --output text > top100.txt

  

Alternatively, use the following shell command, pasing in the text file. : 
xargs -n1 -I{} aws s3 rm s3://edgar-edge-raw/{} --quiet < fulldataset_ordered_top10000.txt

where each line in the .txt file looks like: 
raw/2025/0001558370-25-002017.json

'''

import boto3
from tqdm import tqdm

# Configuration
BUCKET = 'edgar-edge-raw'
KEYS_FILE = 'top100largest.txt'  # The file containing the top 100 keys, one per whitespace

def load_keys(file_path):
    """Read S3 object keys from the provided file."""
    with open(file_path, 'r') as f:
        content = f.read()
    # Split on any whitespace to get individual keys
    return content.split()

def delete_objects(bucket, keys):
    """Delete each key from the specified S3 bucket, showing progress."""
    s3 = boto3.client('s3')
    for key in tqdm(keys, desc='Deleting objects'):
        s3.delete_object(Bucket=bucket, Key=key)

if __name__ == '__main__':
    keys_to_delete = load_keys(KEYS_FILE)
    print(f'Loaded {len(keys_to_delete)} keys to delete.')
    delete_objects(BUCKET, keys_to_delete)
    print('Deletion complete!')
